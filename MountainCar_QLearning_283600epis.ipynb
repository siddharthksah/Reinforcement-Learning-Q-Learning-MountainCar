{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MountaincarQAgent():\n",
    "    def __init__(self, buckets=(12, 12), num_episodes=500000, min_epsilon=0.01, discount=0.99, decay=100, force=True):\n",
    "        self.buckets = buckets\n",
    "        self.num_episodes = num_episodes\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay = decay\n",
    "\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        self.upper_bounds = [self.env.observation_space.high[0], self.env.observation_space.high[1]]\n",
    "        self.lower_bounds = [self.env.observation_space.low[0], self.env.observation_space.low[1]]\n",
    "        \n",
    "        ## Concatination of tuples to get shape (12,12,3) for buckets=(12, 12)\n",
    "        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))        \n",
    "        \n",
    "        self.learning_rate = 0.008\n",
    "        \n",
    "        self.threshold = self.env.spec.reward_threshold\n",
    "        print('threshold: ', self.threshold)\n",
    "\n",
    "    def discretize_state(self, obs):\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            scaling = (obs[i] + abs(self.lower_bounds[i])) / (self.upper_bounds[i] - self.lower_bounds[i])\n",
    "            new_obs = int(round((self.buckets[i] - 1) * scaling))\n",
    "            new_obs = min(self.buckets[i] - 1, max(0, new_obs))\n",
    "            discretized.append(new_obs)\n",
    "        return tuple(discretized)\n",
    "\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if (np.random.random() < self.epsilon):\n",
    "            return self.env.action_space.sample() \n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])\n",
    "\n",
    "    def update_q(self, state, action, reward, new_state):\n",
    "        self.Q_table[state][action] += \\\n",
    "           self.learning_rate * (reward + self.discount * np.max(self.Q_table[new_state]) - self.Q_table[state][action])\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        scores_deque = deque(maxlen=100)\n",
    "        scores_array = []\n",
    "        avg_scores_array = []  \n",
    "        print_every = 400\n",
    "        time_start = time.time()\n",
    "        \n",
    "        for i_episode in range(self.num_episodes):\n",
    "            current_state = self.discretize_state(self.env.reset())\n",
    "\n",
    "            self.epsilon = self.get_epsilon(i_episode)\n",
    "            done = False\n",
    "            \n",
    "            episode_reward = 0\n",
    "            time_step = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(current_state)\n",
    "                obs, reward, done, _ = self.env.step(action)   \n",
    "                new_state = self.discretize_state(obs)\n",
    "                self.update_q(current_state, action, reward, new_state)\n",
    "                current_state = new_state\n",
    "                time_step += 1\n",
    "                episode_reward += reward\n",
    "                \n",
    "            scores_deque.append(episode_reward)\n",
    "            scores_array.append(episode_reward)\n",
    "            \n",
    "            avg_score = np.mean(scores_deque)\n",
    "            avg_scores_array.append(avg_score)\n",
    "            \n",
    "            s = (int)(time.time() - time_start)\n",
    "            \n",
    "            if i_episode % print_every == 0 and i_episode > 0:                \n",
    "                print('Episode: {}, Timesteps:  {}, Score: {:5},  Avg.Score: {:.2f}, eps-greedy: {:5.2f}, Time: {:02}:{:02}:{:02}'.\\\n",
    "                    format(i_episode, time_step, episode_reward, avg_score, self.epsilon, s//3600, s%3600//60, s%60))    \n",
    "                \n",
    "            if avg_score >= self.threshold: \n",
    "                print('\\n Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'. \\\n",
    "                    format(i_episode, np.mean(scores_deque)))\n",
    "                break                                \n",
    "\n",
    "        print('Finished training!')\n",
    "        \n",
    "        return scores_array, avg_scores_array\n",
    "\n",
    "    def run(self):\n",
    "        self.env = gym.wrappers.Monitor(self.env,'Mountaincar', force=True)\n",
    "        t = 0\n",
    "        done = False\n",
    "        current_state = self.discretize_state(self.env.reset())\n",
    "        while not done:\n",
    "                self.env.render()\n",
    "                t = t+1\n",
    "                action = self.choose_action(current_state)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(obs)\n",
    "                current_state = new_state\n",
    "            \n",
    "        return t\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  -110.0\n",
      "Episode: 400, Timesteps:  200, Score: -200.0,  Avg.Score: -200.00, eps-greedy:  0.40, Time: 00:00:05\n",
      "Episode: 800, Timesteps:  200, Score: -200.0,  Avg.Score: -200.00, eps-greedy:  0.10, Time: 00:00:11\n",
      "Episode: 1200, Timesteps:  200, Score: -200.0,  Avg.Score: -200.00, eps-greedy:  0.01, Time: 00:00:16\n",
      "Episode: 1600, Timesteps:  200, Score: -200.0,  Avg.Score: -197.64, eps-greedy:  0.01, Time: 00:00:21\n",
      "Episode: 2000, Timesteps:  200, Score: -200.0,  Avg.Score: -198.59, eps-greedy:  0.01, Time: 00:00:27\n",
      "Episode: 2400, Timesteps:  200, Score: -200.0,  Avg.Score: -198.01, eps-greedy:  0.01, Time: 00:00:32\n",
      "Episode: 2800, Timesteps:  121, Score: -121.0,  Avg.Score: -187.99, eps-greedy:  0.01, Time: 00:00:38\n",
      "Episode: 3200, Timesteps:  122, Score: -122.0,  Avg.Score: -185.60, eps-greedy:  0.01, Time: 00:00:43\n",
      "Episode: 3600, Timesteps:  200, Score: -200.0,  Avg.Score: -197.88, eps-greedy:  0.01, Time: 00:00:48\n",
      "Episode: 4000, Timesteps:  200, Score: -200.0,  Avg.Score: -197.24, eps-greedy:  0.01, Time: 00:00:54\n",
      "Episode: 4400, Timesteps:  168, Score: -168.0,  Avg.Score: -187.69, eps-greedy:  0.01, Time: 00:00:59\n",
      "Episode: 4800, Timesteps:  200, Score: -200.0,  Avg.Score: -198.62, eps-greedy:  0.01, Time: 00:01:04\n",
      "Episode: 5200, Timesteps:  175, Score: -175.0,  Avg.Score: -185.79, eps-greedy:  0.01, Time: 00:01:09\n",
      "Episode: 5600, Timesteps:  172, Score: -172.0,  Avg.Score: -181.06, eps-greedy:  0.01, Time: 00:01:14\n",
      "Episode: 6000, Timesteps:  180, Score: -180.0,  Avg.Score: -194.62, eps-greedy:  0.01, Time: 00:01:20\n",
      "Episode: 6400, Timesteps:  200, Score: -200.0,  Avg.Score: -198.29, eps-greedy:  0.01, Time: 00:01:25\n",
      "Episode: 6800, Timesteps:  200, Score: -200.0,  Avg.Score: -170.00, eps-greedy:  0.01, Time: 00:01:31\n",
      "Episode: 7200, Timesteps:  182, Score: -182.0,  Avg.Score: -173.96, eps-greedy:  0.01, Time: 00:01:36\n",
      "Episode: 7600, Timesteps:  171, Score: -171.0,  Avg.Score: -180.88, eps-greedy:  0.01, Time: 00:01:41\n",
      "Episode: 8000, Timesteps:  200, Score: -200.0,  Avg.Score: -191.95, eps-greedy:  0.01, Time: 00:01:47\n",
      "Episode: 8400, Timesteps:  200, Score: -200.0,  Avg.Score: -179.72, eps-greedy:  0.01, Time: 00:01:52\n",
      "Episode: 8800, Timesteps:  200, Score: -200.0,  Avg.Score: -159.14, eps-greedy:  0.01, Time: 00:01:57\n",
      "Episode: 9200, Timesteps:  162, Score: -162.0,  Avg.Score: -162.98, eps-greedy:  0.01, Time: 00:02:01\n",
      "Episode: 9600, Timesteps:  159, Score: -159.0,  Avg.Score: -168.48, eps-greedy:  0.01, Time: 00:02:06\n",
      "Episode: 10000, Timesteps:  172, Score: -172.0,  Avg.Score: -166.84, eps-greedy:  0.01, Time: 00:02:11\n",
      "Episode: 10400, Timesteps:  198, Score: -198.0,  Avg.Score: -160.11, eps-greedy:  0.01, Time: 00:02:16\n",
      "Episode: 10800, Timesteps:  200, Score: -200.0,  Avg.Score: -162.56, eps-greedy:  0.01, Time: 00:02:21\n",
      "Episode: 11200, Timesteps:  182, Score: -182.0,  Avg.Score: -169.77, eps-greedy:  0.01, Time: 00:02:26\n",
      "Episode: 11600, Timesteps:  157, Score: -157.0,  Avg.Score: -164.39, eps-greedy:  0.01, Time: 00:02:30\n",
      "Episode: 12000, Timesteps:  149, Score: -149.0,  Avg.Score: -166.71, eps-greedy:  0.01, Time: 00:02:35\n",
      "Episode: 12400, Timesteps:  153, Score: -153.0,  Avg.Score: -161.78, eps-greedy:  0.01, Time: 00:02:40\n",
      "Episode: 12800, Timesteps:  174, Score: -174.0,  Avg.Score: -161.15, eps-greedy:  0.01, Time: 00:02:44\n",
      "Episode: 13200, Timesteps:  113, Score: -113.0,  Avg.Score: -147.91, eps-greedy:  0.01, Time: 00:02:49\n",
      "Episode: 13600, Timesteps:  191, Score: -191.0,  Avg.Score: -170.85, eps-greedy:  0.01, Time: 00:02:53\n",
      "Episode: 14000, Timesteps:  164, Score: -164.0,  Avg.Score: -162.02, eps-greedy:  0.01, Time: 00:02:58\n",
      "Episode: 14400, Timesteps:  200, Score: -200.0,  Avg.Score: -166.67, eps-greedy:  0.01, Time: 00:03:03\n",
      "Episode: 14800, Timesteps:  158, Score: -158.0,  Avg.Score: -147.07, eps-greedy:  0.01, Time: 00:03:07\n",
      "Episode: 15200, Timesteps:  154, Score: -154.0,  Avg.Score: -162.72, eps-greedy:  0.01, Time: 00:03:11\n",
      "Episode: 15600, Timesteps:  143, Score: -143.0,  Avg.Score: -153.15, eps-greedy:  0.01, Time: 00:03:16\n",
      "Episode: 16000, Timesteps:  176, Score: -176.0,  Avg.Score: -165.22, eps-greedy:  0.01, Time: 00:03:21\n",
      "Episode: 16400, Timesteps:  179, Score: -179.0,  Avg.Score: -170.84, eps-greedy:  0.01, Time: 00:03:25\n",
      "Episode: 16800, Timesteps:  115, Score: -115.0,  Avg.Score: -169.88, eps-greedy:  0.01, Time: 00:03:30\n",
      "Episode: 17200, Timesteps:  156, Score: -156.0,  Avg.Score: -165.93, eps-greedy:  0.01, Time: 00:03:35\n",
      "Episode: 17600, Timesteps:  114, Score: -114.0,  Avg.Score: -146.42, eps-greedy:  0.01, Time: 00:03:39\n",
      "Episode: 18000, Timesteps:  160, Score: -160.0,  Avg.Score: -147.30, eps-greedy:  0.01, Time: 00:03:44\n",
      "Episode: 18400, Timesteps:  191, Score: -191.0,  Avg.Score: -143.39, eps-greedy:  0.01, Time: 00:03:48\n",
      "Episode: 18800, Timesteps:  162, Score: -162.0,  Avg.Score: -139.23, eps-greedy:  0.01, Time: 00:03:52\n",
      "Episode: 19200, Timesteps:  151, Score: -151.0,  Avg.Score: -157.73, eps-greedy:  0.01, Time: 00:03:56\n",
      "Episode: 19600, Timesteps:  107, Score: -107.0,  Avg.Score: -153.28, eps-greedy:  0.01, Time: 00:04:00\n",
      "Episode: 20000, Timesteps:  167, Score: -167.0,  Avg.Score: -157.05, eps-greedy:  0.01, Time: 00:04:05\n",
      "Episode: 20400, Timesteps:  116, Score: -116.0,  Avg.Score: -153.39, eps-greedy:  0.01, Time: 00:04:09\n",
      "Episode: 20800, Timesteps:  141, Score: -141.0,  Avg.Score: -171.30, eps-greedy:  0.01, Time: 00:04:13\n",
      "Episode: 21200, Timesteps:  163, Score: -163.0,  Avg.Score: -175.57, eps-greedy:  0.01, Time: 00:04:18\n",
      "Episode: 21600, Timesteps:  200, Score: -200.0,  Avg.Score: -170.14, eps-greedy:  0.01, Time: 00:04:23\n",
      "Episode: 22000, Timesteps:  154, Score: -154.0,  Avg.Score: -167.34, eps-greedy:  0.01, Time: 00:04:27\n",
      "Episode: 22400, Timesteps:  187, Score: -187.0,  Avg.Score: -166.38, eps-greedy:  0.01, Time: 00:04:32\n",
      "Episode: 22800, Timesteps:  168, Score: -168.0,  Avg.Score: -164.25, eps-greedy:  0.01, Time: 00:04:37\n",
      "Episode: 23200, Timesteps:  175, Score: -175.0,  Avg.Score: -166.78, eps-greedy:  0.01, Time: 00:04:41\n",
      "Episode: 23600, Timesteps:  166, Score: -166.0,  Avg.Score: -167.54, eps-greedy:  0.01, Time: 00:04:46\n",
      "Episode: 24000, Timesteps:  142, Score: -142.0,  Avg.Score: -165.48, eps-greedy:  0.01, Time: 00:04:51\n",
      "Episode: 24400, Timesteps:  152, Score: -152.0,  Avg.Score: -163.35, eps-greedy:  0.01, Time: 00:04:55\n",
      "Episode: 24800, Timesteps:  200, Score: -200.0,  Avg.Score: -177.95, eps-greedy:  0.01, Time: 00:05:00\n",
      "Episode: 25200, Timesteps:  162, Score: -162.0,  Avg.Score: -161.54, eps-greedy:  0.01, Time: 00:05:04\n",
      "Episode: 25600, Timesteps:  152, Score: -152.0,  Avg.Score: -152.92, eps-greedy:  0.01, Time: 00:05:09\n",
      "Episode: 26000, Timesteps:  165, Score: -165.0,  Avg.Score: -160.85, eps-greedy:  0.01, Time: 00:05:13\n",
      "Episode: 26400, Timesteps:  169, Score: -169.0,  Avg.Score: -158.96, eps-greedy:  0.01, Time: 00:05:18\n",
      "Episode: 26800, Timesteps:  112, Score: -112.0,  Avg.Score: -149.56, eps-greedy:  0.01, Time: 00:05:22\n",
      "Episode: 27200, Timesteps:  111, Score: -111.0,  Avg.Score: -158.07, eps-greedy:  0.01, Time: 00:05:26\n",
      "Episode: 27600, Timesteps:  109, Score: -109.0,  Avg.Score: -149.99, eps-greedy:  0.01, Time: 00:05:31\n",
      "Episode: 28000, Timesteps:  167, Score: -167.0,  Avg.Score: -162.85, eps-greedy:  0.01, Time: 00:05:35\n",
      "Episode: 28400, Timesteps:  167, Score: -167.0,  Avg.Score: -167.48, eps-greedy:  0.01, Time: 00:05:40\n",
      "Episode: 28800, Timesteps:  156, Score: -156.0,  Avg.Score: -153.27, eps-greedy:  0.01, Time: 00:05:44\n",
      "Episode: 29200, Timesteps:  168, Score: -168.0,  Avg.Score: -154.31, eps-greedy:  0.01, Time: 00:05:49\n",
      "Episode: 29600, Timesteps:  151, Score: -151.0,  Avg.Score: -158.61, eps-greedy:  0.01, Time: 00:05:53\n",
      "Episode: 30000, Timesteps:  155, Score: -155.0,  Avg.Score: -158.18, eps-greedy:  0.01, Time: 00:05:58\n",
      "Episode: 30400, Timesteps:  172, Score: -172.0,  Avg.Score: -163.89, eps-greedy:  0.01, Time: 00:06:02\n",
      "Episode: 30800, Timesteps:  192, Score: -192.0,  Avg.Score: -161.10, eps-greedy:  0.01, Time: 00:06:07\n",
      "Episode: 31200, Timesteps:  110, Score: -110.0,  Avg.Score: -157.24, eps-greedy:  0.01, Time: 00:06:11\n",
      "Episode: 31600, Timesteps:  167, Score: -167.0,  Avg.Score: -149.90, eps-greedy:  0.01, Time: 00:06:16\n",
      "Episode: 32000, Timesteps:  175, Score: -175.0,  Avg.Score: -144.88, eps-greedy:  0.01, Time: 00:06:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 32400, Timesteps:  157, Score: -157.0,  Avg.Score: -153.70, eps-greedy:  0.01, Time: 00:06:24\n",
      "Episode: 32800, Timesteps:  152, Score: -152.0,  Avg.Score: -160.58, eps-greedy:  0.01, Time: 00:06:29\n",
      "Episode: 33200, Timesteps:  117, Score: -117.0,  Avg.Score: -160.75, eps-greedy:  0.01, Time: 00:06:33\n",
      "Episode: 33600, Timesteps:  120, Score: -120.0,  Avg.Score: -154.17, eps-greedy:  0.01, Time: 00:06:38\n",
      "Episode: 34000, Timesteps:  161, Score: -161.0,  Avg.Score: -164.43, eps-greedy:  0.01, Time: 00:06:42\n",
      "Episode: 34400, Timesteps:  180, Score: -180.0,  Avg.Score: -154.68, eps-greedy:  0.01, Time: 00:06:46\n",
      "Episode: 34800, Timesteps:  148, Score: -148.0,  Avg.Score: -170.50, eps-greedy:  0.01, Time: 00:06:51\n",
      "Episode: 35200, Timesteps:  153, Score: -153.0,  Avg.Score: -155.72, eps-greedy:  0.01, Time: 00:06:56\n",
      "Episode: 35600, Timesteps:  119, Score: -119.0,  Avg.Score: -149.14, eps-greedy:  0.01, Time: 00:07:01\n",
      "Episode: 36000, Timesteps:  197, Score: -197.0,  Avg.Score: -176.04, eps-greedy:  0.01, Time: 00:07:06\n",
      "Episode: 36400, Timesteps:  200, Score: -200.0,  Avg.Score: -196.56, eps-greedy:  0.01, Time: 00:07:11\n",
      "Episode: 36800, Timesteps:  164, Score: -164.0,  Avg.Score: -170.76, eps-greedy:  0.01, Time: 00:07:17\n",
      "Episode: 37200, Timesteps:  177, Score: -177.0,  Avg.Score: -165.80, eps-greedy:  0.01, Time: 00:07:22\n",
      "Episode: 37600, Timesteps:  148, Score: -148.0,  Avg.Score: -160.32, eps-greedy:  0.01, Time: 00:07:26\n",
      "Episode: 38000, Timesteps:  152, Score: -152.0,  Avg.Score: -165.71, eps-greedy:  0.01, Time: 00:07:31\n",
      "Episode: 38400, Timesteps:  118, Score: -118.0,  Avg.Score: -143.43, eps-greedy:  0.01, Time: 00:07:35\n",
      "Episode: 38800, Timesteps:  152, Score: -152.0,  Avg.Score: -149.86, eps-greedy:  0.01, Time: 00:07:40\n",
      "Episode: 39200, Timesteps:  140, Score: -140.0,  Avg.Score: -146.55, eps-greedy:  0.01, Time: 00:07:44\n",
      "Episode: 39600, Timesteps:  163, Score: -163.0,  Avg.Score: -150.95, eps-greedy:  0.01, Time: 00:07:48\n",
      "Episode: 40000, Timesteps:  149, Score: -149.0,  Avg.Score: -143.56, eps-greedy:  0.01, Time: 00:07:52\n",
      "Episode: 40400, Timesteps:  145, Score: -145.0,  Avg.Score: -151.11, eps-greedy:  0.01, Time: 00:07:56\n",
      "Episode: 40800, Timesteps:  148, Score: -148.0,  Avg.Score: -148.11, eps-greedy:  0.01, Time: 00:08:00\n",
      "Episode: 41200, Timesteps:  151, Score: -151.0,  Avg.Score: -152.68, eps-greedy:  0.01, Time: 00:08:05\n",
      "Episode: 41600, Timesteps:  139, Score: -139.0,  Avg.Score: -157.87, eps-greedy:  0.01, Time: 00:08:09\n",
      "Episode: 42000, Timesteps:  135, Score: -135.0,  Avg.Score: -150.43, eps-greedy:  0.01, Time: 00:08:14\n",
      "Episode: 42400, Timesteps:  133, Score: -133.0,  Avg.Score: -138.36, eps-greedy:  0.01, Time: 00:08:17\n"
     ]
    }
   ],
   "source": [
    "agent = MountaincarQAgent()\n",
    "scores, avg_scores = agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "print('length of scores: ', len(scores), ', len of avg_scores: ', len(avg_scores))\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores, label=\"Score\")\n",
    "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1)) \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 110\n"
     ]
    }
   ],
   "source": [
    "t = agent.run()\n",
    "print(\"Time\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl] *",
   "language": "python",
   "name": "conda-env-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
